{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy NetFlow Network Intrusion Detection System Model Package from AWS Marketplace \n",
    "---\n",
    "### Product Overview: \n",
    "This product is capable of detecting network attacks within a NetFlow network protocol using machine learning techniques. The ML multiclass trained model can detect 4 main network traffic categories: Benign, Brute Force, DoS & DDoS.\n",
    "\n",
    "**Detection categories:**\n",
    "- Benign: If the inference output is *Benign* that means that the input NetFlow record does not belongs to any attack class, in other words, is normal traffic.\n",
    "- Brute Force: If the inference output from an input Netflow record is *Brute Force*, that record belongs to possible Brute Force attack.\n",
    "- DoS: If the inference output from an input Netflow record is *DoS*, that record belongs to a possible Denial of Service attack.\n",
    "- DDoS: If the inference output from an input Netflow record is *DDoS*, that record belongs to a possible Distributed Denial of Service attack.\n",
    "\n",
    "**Expected input format:** The Endpoint must receive any of the 2 allowed formats: CSV & JSON. The input data must not contain the data header (field names)\n",
    "\n",
    "**Requiered input data fields:** The Machine Learning model was trained with all the Cisco Netflow V5 fields, therefore to perform an inference the same fields need to be fed to the model.\n",
    "\n",
    "The fields must be in the following order: 'srcaddr', 'dstaddr', 'nexthop',\n",
    "                           'input', 'output', 'dPkts',\n",
    "                           'dOctets', 'first', 'last',\n",
    "                           'srcport', 'dstport',\n",
    "                           'tcp_flags', 'prot', 'tos',\n",
    "                           'src_as', 'dst_as', 'src_mask',\n",
    "                           'dst_mask'.\n",
    "                           \n",
    "More information about the description and meaning of each field in the following link: https://www.ibm.com/docs/en/npi/1.3.0?topic=versions-netflow-v5-formats\n",
    "\n",
    "\n",
    "**Expected output format:** The inference output received as the endpoint response will be in JSON format and it will contain all the input data plus two new features which are the respective prediction for each record alongside 'confidence' that means the percentage of confidence the model used to output the final prediction between all the possible categories.\n",
    "\n",
    "In case of a wrong input data format, the model will avoid the inference of the affected network flows, appending an error message at the end of the row. The error message could be:\n",
    "- “Wrong data type” if the values in each log are not numbers\n",
    "- “Wrong number of columns” if the number of columns for a given log is different from the allowed 18 NetFlow V5\n",
    "- “Wrong IP address” if some of the IP addresses are not correctly composed (only IPV4 allowed)\n",
    "- “Null data” when some field/fields are empty\n",
    "\n",
    "**Model performance:**\n",
    "   -  Overall accuracy (binary detection): 84%\n",
    "   -  Overall recall (binary detection): 84%\n",
    "   -  Multiclass accuracy (multiclass detection): 78%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model package deploy example\n",
    "This sample notebook shows you how to deploy <font color='red'> For Seller to update:[Title_of_your_ML Model](Provide link to your marketplace listing of your product)</font> using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "#### Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to <font color='red'> For Seller to update:[Title_of_your_ML Model](Provide link to your marketplace listing of your product)</font>. If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "#### Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page <font color='red'> For Seller to update:[Title_of_your_product](Provide link to your marketplace listing of your product).</font>\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = \"arn:aws:sagemaker:us-east-1:452490241637:model-package/netflow-threats-detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "runtime = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>For Seller to update: update values for four variables in following cell. \n",
    "Specify a model/endpoint name using only alphanumeric characters. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"netlow-ai\"\n",
    "\n",
    "real_time_inference_instance_type = (\n",
    "    \"ml.t2.medium\"\n",
    ")\n",
    "batch_transform_inference_instance_type = (\n",
    "    \"ml.m5.large\"\n",
    ")\n",
    "\n",
    "#real_time_inference_instance_type = \"ml.t2.medium\"\n",
    "#batch_transform_inference_instance_type = \"ml.m5.large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "# Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Local variable stored payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Cisco Netflow V5 column names\n",
    "netflow_v5_column_names = ['srcaddr', 'dstaddr', 'nexthop',\n",
    "                           'input', 'output', 'dPkts',\n",
    "                           'dOctets', 'first', 'last',\n",
    "                           'srcport', 'dstport',\n",
    "                           'tcp_flags', 'prot', 'tos',\n",
    "                           'src_as', 'dst_as', 'src_mask',\n",
    "                           'dst_mask', 'prediction', 'confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and show the csv format example data\n",
    "data = pd.read_csv('data/input/real-time/example-data.csv',\n",
    "                   names = netflow_v5_column_names[:-2],\n",
    "                   sep = ',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to csv format again and define data content type\n",
    "input_data = data.to_csv(header=None, index = None)\n",
    "content_type = 'text/csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File stored payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from the example data file then define the data content type. There are 2 options of allowed input data, CSV format or JSON format. Uncomment the option you want to try.\n",
    "input_data_file = 'data/input/real-time/netflow-v5-real-time-sample.csv'\n",
    "#input_data_file = 'data/input/real-time/netflow-flows.json'\n",
    "\n",
    "content_type_file = \"text/csv\"\n",
    "#content_type_file = \"application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output target file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path where the result will be stored\n",
    "output_file_name = 'data/output/example-data-out.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Machine Learning model accepts a payload and returns an inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inference from local variable. With BOTO3 sdk endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run inference using the locally stored payload, then print the result in JSON format\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=model_name,\n",
    "    Body=input_data,\n",
    "    ContentType=content_type,\n",
    ")\n",
    "response_body = json.loads(response['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inference from file stored payload. With CLI endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ContentType\": \"text/html; charset=utf-8\",\n",
      "    \"InvokedProductionVariant\": \"AllTraffic\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Run inference using only the path in which the payload is stored. Output is then saved to the output_file_name file.\n",
    "!aws sagemaker-runtime invoke-endpoint \\\n",
    "    --endpoint-name $model_name \\\n",
    "    --body fileb://$input_data_file \\\n",
    "    --content-type $content_type_file \\\n",
    "    --region $sagemaker_session.boto_region_name \\\n",
    "    $output_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Visualize output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the output generated by real-time inference. The final column 'prediction' is the result of the inference alongside 'confidence' which means the percentage of confidence the model used to output the final prediction between all the possible categories.\n",
    "\n",
    "- From the BOTO3 inference endpoint request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize endpoint output as a Pandas Dataframe.\n",
    "pd.DataFrame(response_body['prediction'], columns = netflow_v5_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the CLI inference endpoint request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize endpoint output as a Pandas Dataframe.\n",
    "with open(output_file_name, 'r') as j:\n",
    "     contents = json.loads(j.read())['prediction']\n",
    "output_data = pd.DataFrame(contents, columns = netflow_v5_column_names)\n",
    "output_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define bucket name and prefix to store the input/output batch data in AWS S3 service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define bucket name and prefix to allow batch-type inference. If None, bucket will be created with default name and location.\n",
    "bucket_name = 'your/bucket/name'\n",
    "key_prefix = 'your/key/prefix'\n",
    "\n",
    "#define output path inside defined bucket to store the inference output\n",
    "output_path = 'your/bucket/name/and/key/prefix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path where input batch payload is located\n",
    "transform_input_folder = \"data/input/batch\"\n",
    "\n",
    "# upload the batch-transform job input files to S3\n",
    "transform_input = sagemaker_session.upload_data(transform_input_folder,\n",
    "                                                bucket=bucket_name,\n",
    "                                                key_prefix=key_prefix)\n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch-transform job\n",
    "transformer = model.transformer(1,\n",
    "                                batch_transform_inference_instance_type,\n",
    "                                output_path = output_path)\n",
    "transformer.transform(transform_input,\n",
    "                      content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is in JSON format, view the output file by uncommenting and running following command. Otherwise go to S3, download the file and open it using appropriate editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_url = urlparse(transformer.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "file_key = \"{}/{}.out\".format(parsed_url.path[1:], 'example-data-1.txt'.split(\"/\")[-1])\n",
    "print(file_key)\n",
    "s3_client = sagemaker_session.boto_session.client(\"s3\")\n",
    "\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "response_bytes = response[\"Body\"].read().decode(\"utf-8\")\n",
    "print(response_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
